# -*- coding: utf-8 -*-
"""Torch_2023_10_03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zYgpMBrMyVpfpUPY_rZAWwyTZv50Wo-p
"""

!pip install -q torcheval

from google.colab import drive
drive.mount('/content/drive')

!unzip -q /content/drive/MyDrive/SRH/SRH_jpg_224.zip

import torch.nn as nn
import math
from torch.nn import Parameter
from torchvision.models import resnet18, ResNet50_Weights, resnet50
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from random import shuffle
import sys
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import cv2, os
import pandas as pd
from torch.utils.data import DataLoader
import albumentations as album
from torcheval.metrics import MulticlassAccuracy
from torch.nn import functional as F
import torch
import torch.nn.init as init
from torchvision import models
import json
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

json_file = open('/content/drive/MyDrive/SRH/train_val_split.json')
train_val_split = json.load(json_file)

df = pd.read_json('/content/drive/MyDrive/SRH/opensrh.json')

train_patients = df[train_val_split['train']]
valid_patients = df[train_val_split['val']]


def read_data(df):
    patches = []
    for p in df.keys().tolist():
        class_name = df[p]['class']
        for id in df[p]['slides'].keys():
            for cat in ['tumor_patches', 'normal_patches', 'nondiagnostic_patches']:
                for path in df[p]['slides'][id][cat]:
                    new_path = path.replace('studies', '/content/drive/MyDrive/SRH/unzip')
                    patches.append([new_path, class_name, cat])
    df = pd.DataFrame(patches, columns=['path', 'class', 'sub_class'])
    return df

train_df = read_data(train_patients)
valid_df = read_data(valid_patients)

train_df = train_df.drop(train_df[train_df['sub_class'] == 'nondiagnostic_patches'].index)
train_df = train_df.reset_index(drop=True)

valid_df = valid_df.drop(valid_df[valid_df['sub_class'] == 'nondiagnostic_patches'].index)
valid_df = valid_df.reset_index(drop=True)

train_df['path'] = train_df['path'].apply(lambda x: x.replace('/content/drive/MyDrive/SRH/unzip/', 'SRH_jpg_224/'))
train_df['path'] = train_df['path'].apply(lambda x: x.replace('.tif', '.jpg'))

valid_df['path'] = valid_df['path'].apply(lambda x: x.replace('/content/drive/MyDrive/SRH/unzip/', 'SRH_jpg_224/'))
valid_df['path'] = valid_df['path'].apply(lambda x: x.replace('.tif', '.jpg'))

class_dict = {'hgg':0,
               'lgg':1,
               'mening': 2,
               'metast': 3,
               'pituita': 4,
               'schwan': 5,
               'normal': 6,
               'normal_tumor': 7}

k= 4
num_classes= 8
learning_rate= 0.001
image_size = 224
batch_size = 512
epochs = 200

device = ('cuda' if torch.cuda.is_available() else 'cpu')

for i in range(len(train_df)):
    if train_df['sub_class'][i]=='normal_patches':
        train_df['class'][i] = 'normal_tumor'
for i in range(len(valid_df)):
    if valid_df['sub_class'][i]=='normal_patches':
        valid_df['class'][i] = 'normal_tumor'

train_transforms = [
#    album.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),
    # album.PixelDropout (dropout_prob=0.3, per_channel=False, drop_value=0, mask_drop_value=None, always_apply=False, p=0.5),
    # album.PiecewiseAffine (scale=(0.03, 0.05), nb_rows=4, nb_cols=4, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode='constant', absolute_scale=False, always_apply=False, keypoints_threshold=0.01, p=0.5),
    album.HorizontalFlip(always_apply=False, p=0.5),
    # album.Rotate (limit=[-15,15], interpolation=1, border_mode=4, value=None, mask_value=None, rotate_method='largest_box', crop_border=False, always_apply=False, p=0.5),
    # album.Defocus (radius=(10, 20), alias_blur=(0.1, 0.5), always_apply=False, p=0.5),
    album.Affine (scale=None, translate_percent=None, translate_px=None, rotate=None, shear=None, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode=0, fit_output=False, keep_ratio=False, always_apply=False, p=0.5),
    album.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1),
    ]
train_transform = album.Compose(train_transforms)
def train_augment(image):
    aug_image = train_transform(image=image)['image']
    return aug_image

valid_transforms = [
    album.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1),]
valid_transforms = album.Compose(valid_transforms)
def valid_augment(image):
    aug_image = valid_transforms(image=image)['image']
    return aug_image

class DATASET(torch.utils.data.Dataset):
    def __init__(self, df, transform, class_dict):
        self.df = df
        self.transform = transform
        self.class_dict = class_dict

    def __getitem__(self, i):
        image = cv2.imread(self.df['path'][i])
        image = cv2.resize(image, (224,224))
        image =  self.transform(image)
        image = image.transpose(2, 0, 1)

        label = self.df['class'][i]
        label = self.class_dict[label]
        target = np.zeros((num_classes))
        target[label] = 1
        target = target.astype('float32')
        return image, target

    def __len__(self):
        return len(self.df)

train_dataset= DATASET(train_df, train_augment, class_dict)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
valid_dataset= DATASET(valid_df, valid_augment, class_dict)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

class GlobalAttentionBlock(nn.Module):
    def __init__(self, num_features):
        super(GlobalAttentionBlock, self).__init__()
        self.block = nn.Sequential(
            nn.AvgPool2d((7,7)),
            nn.Conv2d(num_features, num_features, kernel_size=1, padding='same'),
            nn.ReLU(),
            nn.Conv2d(num_features, num_features, kernel_size=1, padding='same'),
            nn.Sigmoid(),
        )

    def forward(self, x):
        out = self.block(x)
        C_A = torch.multiply(out, x)
        out = torch.mean(C_A, dim=1, keepdim=True)
        out = torch.sigmoid(out)
        S_A = torch.multiply(out, C_A)
        return S_A

class CategoryAttentionBlock(nn.Module):
    def __init__(self, num_classes, k, num_features):
        super(CategoryAttentionBlock, self).__init__()

        self.num_classes = num_classes
        self.k = k
        self.block = nn.Sequential(
            nn.Conv2d(num_features, k*num_classes, kernel_size=1, padding='same'),
            nn.BatchNorm2d(k*num_classes),
            nn.ReLU(),
        )
        self.pool = nn.AdaptiveMaxPool2d((1, 1))

    def forward(self, x, inputs):
        F = self.block(x)
        out = self.pool(F)

        out = torch.reshape(out, (out.shape[0], self.k, self.num_classes))
        S = torch.mean(out, dim=1, keepdim=False)
        out = torch.reshape(F, (F.shape[0], self.num_classes, self.k, 7, 7))
        out = torch.mean(out, dim=2, keepdim=False)
        S = torch.reshape(S, (S.shape[0], self.num_classes, 1, 1))
        out = torch.multiply(S, out)
        out = torch.mean(out, dim=1, keepdim=True)
        semantic = torch.multiply(inputs, out)
        return semantic

class CABNet(nn.Module):
    def __init__(self, num_classes,k, num_features):
        super(CABNet, self).__init__()

        base_model = resnet18(weights=None)

        self.features = nn.Sequential(*list(base_model.children())[:-2])

        self.global_attention = GlobalAttentionBlock(num_features= num_features)
        self.category_attention = CategoryAttentionBlock(num_classes= num_classes, k=k, num_features=num_features)
        self.fc_block=nn.Sequential(
            nn.Dropout(p=0.5),
            nn.Linear(num_features, num_classes),
            nn.Softmax(dim=1),
        )
        self.avgpool = nn.Sequential(nn.AdaptiveAvgPool2d(1),
                                    nn.Flatten())

    def forward(self, x):
        features = self.features(x)
        x = self.global_attention(features)
        semantic = self.category_attention(x, features)

        semantic = self.avgpool(semantic)

        semantic = self.fc_block(semantic)
        return semantic

model = CABNet(num_classes= num_classes, k=k, num_features=512).to(device)
# a= torch.randn(2,3,224,224).to(device)
# model(a).shape

optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=0.001),])
scheduler= torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1, verbose=True)

criterion = torch.nn.CrossEntropyLoss()

counter = 1
best_valid_loss = np.inf
len_train_loader = len(train_loader)
len_valid_loader = len(valid_loader)
metric = MulticlassAccuracy(num_classes=num_classes).to(device)
history = [['train_loss', 'train_acc', 'valid_loss', 'valid_acc']]
for epoch_ind in range(1, epochs + 1):
    running_loss = 0
    model.train()

    for batch_ind, (images, targets) in enumerate(train_loader):
        targets = targets.to(device)
        preds = model(images.to(device))

        loss = criterion(preds, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss

        train_acc = float(metric.update(preds.argmax(dim=1), targets.argmax(dim=1)).compute())
        sys.stdout.write("\rEpoch {}, Train Loss: {:<4.4f}, Acc {:<6.4f}%, Progress: {:4.2f}". format(epoch_ind, running_loss/(batch_ind+1), train_acc*100, ((batch_ind+1)/len_train_loader)*100))
        sys.stdout.flush()
    train_loss = running_loss/len_train_loader
    metric.reset()

    running_loss = 0
    for batch_ind, (images, targets) in enumerate(valid_loader):
        targets = targets.to(device)
        preds = model(images.to(device))

        loss = criterion(preds, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss

        valid_acc = float(metric.update(preds.argmax(dim=1), targets.argmax(dim=1)).compute())
        sys.stdout.write("\rEpoch {}, Train Loss: {:<4.4f}, Acc {:<6.4f}%, Valid Loss: {:<4.4f}, Acc {:<6.4f}%, Progress: {:4.2f}".format(epoch_ind, train_loss, train_acc*100, running_loss/(batch_ind+1), valid_acc*100, ((batch_ind+1)/len_valid_loader)*100))
        sys.stdout.flush()
    valid_loss = running_loss/len_valid_loader
    metric.reset()


    sys.stdout.flush()
    sys.stdout.write("\r"+" "*200)
    sys.stdout.write("\rEpoch {}, Train Loss: {:<4.4f}, Acc {:<6.4f}%, Valid Loss: {:<4.4f}, Acc {:<6.4f}%". format(epoch_ind, train_loss, train_acc*100, valid_loss, valid_acc*100))
    sys.stdout.write("\r\n")
    sys.stdout.flush()

    history.append([float(train_loss), train_acc, float(valid_loss), valid_acc])
    if valid_loss<best_valid_loss:
        torch.save(model.state_dict(), 'model_weights.pth')
        print('Model saved.')
        best_valid_loss = valid_loss
        counter=1
    elif valid_loss>best_valid_loss:
        counter+=1

    if counter%3==0:
        # model.load_state_dict(torch.load('model_weights.pth'))
        counter=1
        scheduler.step()

sys.stdout.write("\rEpoch {}, Train Loss: {:<4.4f}, Acc {:<6.4f}%, Valid Loss: {:<4.4f}, Acc {:<6.4f}". format(epoch_ind, train_loss, train_acc*100, valid_loss, valid_acc*100))